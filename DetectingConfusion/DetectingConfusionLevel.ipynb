{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DetectingConfusionLevel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXBSVevp_Urh"
      },
      "source": [
        "# DETECTING CONFUSION LEVEL\n",
        "Python code related to my thesis project where I had to conduct an experiment where participants took cognitive tests. During the cognitive tests, we recorded their EEG signals. At the end of each cognitive exercise, we asked the participants to indicate their level of confusion. I then trained models to detect the intensity of confusion (not confused, slightly, moderately and very confused) or the confusion state (no confusion, confusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlZKusY8CBD5"
      },
      "source": [
        "PROGRAMMING AIM: We will use classes, exceptions and follow the design of the MVC model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5lx8fyllf0D"
      },
      "source": [
        "Check if we are using the GPU Alienware\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d8u-pgGfxmW"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "import numpy as np\n",
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRQ1tYCmlkiR"
      },
      "source": [
        "Check if Cuda is installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bl4JDu3loBE",
        "outputId": "0675328d-15ac-4e98-9bb3-88a2cc42af29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTv1TaNOzXvf"
      },
      "source": [
        "Our future methods will catch commun exceptions with this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PteFx1m9x13H"
      },
      "source": [
        "import inspect\n",
        "def ExceptionMessage(exc):\n",
        "  print(\"Exception of type:\", exc.__class__)\n",
        "  print(\"Called by fonction:\", inspect.stack()[1][3])\n",
        "  print(\"Message:\", exc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXhA9dUvNZ-N"
      },
      "source": [
        "# DRIVE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BQwm_MqFKIx"
      },
      "source": [
        "##GoogleDrive Class (model)\n",
        "\n",
        "We are going to create a class with the attributes of the drive, i.e., its URLs so later we can have access to EEG files. \n",
        "\n",
        "```\n",
        "# Note: If we follow the MVC model, the GoogleDrive class is a model.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-qx7PHTHYTz"
      },
      "source": [
        "class GoogleDrive:\n",
        "  \"\"\"Class with the urls of the drive to load its files later on\"\"\"\n",
        "  #Attributes: url of drive, and url of the EEG files\n",
        "  def __init__(self, urlDrive='/content/drive', urlFiles= '/content/drive/My Drive/Colab Notebooks/Data/CognitiveTest/', urlLabels = '../content/drive/My Drive/Colab Notebooks/Data/CognitiveTest/All_exercices.csv'):\n",
        "    self._urlDrive = urlDrive\n",
        "    self._urlFiles = urlFiles\n",
        "    self._urlLabels = urlLabels\n",
        "    \n",
        "  #Get the drive url\n",
        "  def GetUrlDrive(self):\n",
        "    return self._urlDrive\n",
        "  \n",
        "  #Get the url of the EEG signals files\n",
        "  def GetUrlFiles(self):\n",
        "    return self._urlFiles\n",
        "  \n",
        "  #Get the url of the EEG labels\n",
        "  def GetUrlLabel(self):\n",
        "    return self._urlLabels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPG7eoZcOvUt"
      },
      "source": [
        "##ViewDrive Class (view)\n",
        "\n",
        "We are going to create a class to see the files present on the drive.\n",
        "\n",
        "\n",
        "```\n",
        "# Note: Following the MVC model, this class would be a view.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxZhXnDzHwml"
      },
      "source": [
        "import os\n",
        "class ViewDrive:\n",
        "  \"\"\"Class to get a view of the drive (print its files and so on)\"\"\"\n",
        "  def __init__(self, GoogleDrive):\n",
        "    self._GoogleDrive = GoogleDrive\n",
        "    \n",
        "  # See files in drive  \n",
        "  def SeeDataDrive(self):\n",
        "    url = self._GoogleDrive.GetUrlFiles()\n",
        "    if os.path.exists(url):\n",
        "      print(sorted(os.listdir(url)))\n",
        "    else:\n",
        "      print(\"Drive URL not found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ_w25LZm8ZY"
      },
      "source": [
        "##DriveController Class (controller)\n",
        "\n",
        "Then we are going to mount our drive using its URL. We will also display its content. To perform these manipulations we will create the controller class of the drive. The controller uses the GoogleDrive class and the ViewDrive class\n",
        "\n",
        "\n",
        "```\n",
        "# Note: Following the MVC model, this class would be a controller.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLn1dmVRPWUm"
      },
      "source": [
        "from google.colab import drive\n",
        "class DriveController:\n",
        "  \"\"\"Class to perform action on the Drive e.g. mount it so that we can access its files\"\"\"\n",
        "  def __init__(self, GoogleDrive=GoogleDrive()):\n",
        "    self._GoogleDrive = GoogleDrive\n",
        "    self._ViewDrive = ViewDrive(self._GoogleDrive)\n",
        "    \n",
        "  #Load the driver helper to access data later and mount.\n",
        "  def MountDrive(self):\n",
        "    try:\n",
        "      drive.mount(self._GoogleDrive.GetUrlDrive()) #Catch any external errors related to the drive\n",
        "      self._ViewDrive.SeeDataDrive()\n",
        "    except Exception as exc:\n",
        "      ExceptionMessage(exc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUN121edIaxa",
        "outputId": "6c0ca8a1-14bf-42e4-e26a-f83cdde4addd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "DC = DriveController()\n",
        "DC.MountDrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "['All_exercices.csv', 'Graphs', 'Graphs0', 'Graphs1', 'P1_s0ex0.csv', 'P1_s0ex1.csv', 'P1_s0ex2.csv', 'P1_s0ex3.csv', 'P1_s1ex0.csv', 'P1_s1ex1.csv', 'P1_s1ex2.csv', 'P1_s1ex3.csv', 'P1_s2ex0.csv', 'P1_s2ex1.csv', 'P1_s2ex2.csv', 'P1_s2ex3.csv', 'P1_s3ex0.csv', 'P1_s3ex1.csv', 'P1_s3ex2.csv', 'P1_s3ex3.csv', 'P1_s4ex0.csv', 'P1_s4ex1.csv', 'P1_s4ex2.csv', 'P1_s4ex3.csv', 'P2_s0ex0.csv', 'P2_s0ex1.csv', 'P2_s0ex2.csv', 'P2_s0ex3.csv', 'P2_s1ex0.csv', 'P2_s1ex1.csv', 'P2_s1ex2.csv', 'P2_s1ex3.csv', 'P2_s2ex0.csv', 'P2_s2ex1.csv', 'P2_s2ex2.csv', 'P2_s2ex3.csv', 'P2_s3ex0.csv', 'P2_s3ex1.csv', 'P2_s3ex2.csv', 'P2_s3ex3.csv', 'P2_s4ex0.csv', 'P2_s4ex1.csv', 'P2_s4ex2.csv', 'P2_s4ex3.csv', 'P3_s0ex0.csv', 'P3_s0ex1.csv', 'P3_s0ex2.csv', 'P3_s0ex3.csv', 'P3_s1ex0.csv', 'P3_s1ex1.csv', 'P3_s1ex2.csv', 'P3_s1ex3.csv', 'P3_s2ex0.csv', 'P3_s2ex1.csv', 'P3_s2ex2.csv', 'P3_s2ex3.csv', 'P3_s3ex0.csv', 'P3_s3ex1.csv', 'P3_s3ex2.csv', 'P3_s3ex3.csv', 'P3_s4ex0.csv', 'P3_s4ex1.csv', 'P3_s4ex2.csv', 'P3_s4ex3.csv', 'P4_s0ex0.csv', 'P4_s0ex1.csv', 'P4_s0ex2.csv', 'P4_s0ex3.csv', 'P4_s1ex0.csv', 'P4_s1ex1.csv', 'P4_s1ex2.csv', 'P4_s1ex3.csv', 'P4_s2ex0.csv', 'P4_s2ex1.csv', 'P4_s2ex2.csv', 'P4_s2ex3.csv', 'P4_s3ex0.csv', 'P4_s3ex1.csv', 'P4_s3ex2.csv', 'P4_s3ex3.csv', 'P4_s4ex0.csv', 'P4_s4ex1.csv', 'P4_s4ex2.csv', 'P4_s4ex3.csv', 'P5_s0ex0.csv', 'P5_s0ex1.csv', 'P5_s0ex2.csv', 'P5_s0ex3.csv', 'P5_s1ex0.csv', 'P5_s1ex1.csv', 'P5_s1ex2.csv', 'P5_s1ex3.csv', 'P5_s2ex0.csv', 'P5_s2ex1.csv', 'P5_s2ex2.csv', 'P5_s2ex3.csv', 'P5_s3ex0.csv', 'P5_s3ex1.csv', 'P5_s3ex2.csv', 'P5_s3ex3.csv', 'P5_s4ex0.csv', 'P5_s4ex1.csv', 'P5_s4ex2.csv', 'P5_s4ex3.csv', 'P6_s0ex0.csv', 'P6_s0ex1.csv', 'P6_s0ex2.csv', 'P6_s0ex3.csv', 'P6_s1ex0.csv', 'P6_s1ex1.csv', 'P6_s1ex2.csv', 'P6_s1ex3.csv', 'P6_s2ex0.csv', 'P6_s2ex1.csv', 'P6_s2ex2.csv', 'P6_s2ex3.csv', 'P6_s3ex0.csv', 'P6_s3ex1.csv', 'P6_s3ex2.csv', 'P6_s3ex3.csv', 'P6_s4ex0.csv', 'P6_s4ex1.csv', 'P6_s4ex2.csv', 'P6_s4ex3.csv', 'P7_s0ex0.csv', 'P7_s0ex1.csv', 'P7_s0ex2.csv', 'P7_s0ex3.csv', 'P7_s1ex0.csv', 'P7_s1ex1.csv', 'P7_s1ex2.csv', 'P7_s1ex3.csv', 'P7_s2ex0.csv', 'P7_s2ex1.csv', 'P7_s2ex2.csv', 'P7_s2ex3.csv', 'P7_s3ex0.csv', 'P7_s3ex1.csv', 'P7_s3ex2.csv', 'P7_s3ex3.csv', 'P7_s4ex0.csv', 'P7_s4ex1.csv', 'P7_s4ex2.csv', 'P7_s4ex3.csv', 'bpData.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_cuvRKw_7Ut"
      },
      "source": [
        "# EXERCICES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeKoUaenFFhp"
      },
      "source": [
        "##ExercicesList Class (model)\n",
        "\n",
        "We are going going to create the class that contains our exercices data (participants' answers during cognitive tests, time they took to complete the exercise, labels of confusion...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FJIEQ9fz0au"
      },
      "source": [
        "#Load Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class ExercicesList:\n",
        "  \"\"\"Class containing the results of the cognitive exercises used in our experiment\"\"\"\n",
        "  def __init__(self, GoogleDrive=GoogleDrive()):\n",
        "    self._gd = GoogleDrive\n",
        "    self._listExercices = pd.read_csv(self._gd.GetUrlLabel()).fillna(0) #list of cognitive exercices with participants' answers and confusion labels\n",
        "    self._arrayExercices = np.array(self._listExercices)\n",
        "    self._nbParticipants = 7 # number of participants of the experiment\n",
        "    self._nbSeries = 5 # number of series of exercices\n",
        "    self._nbExercices = 4 # number of exercices\n",
        "    self._totalExercices = self._nbParticipants*self._nbSeries*self._nbExercices #10x5x4 = 200ex. performed\n",
        "  \n",
        "  # Get google drive\n",
        "  def GetGD(self):\n",
        "    return self._gd\n",
        "\n",
        "  # Get list of exercices\n",
        "  def GetListExercices(self):\n",
        "    return self._listExercices\n",
        "\n",
        "  # Get list of exercices\n",
        "  def GetListExercices(self):\n",
        "    return self._listExercices\n",
        "  \n",
        "  # Get array of exercices\n",
        "  def GetArrayExercices(self):\n",
        "    return self._arrayExercices\n",
        "\n",
        "  # Get number of participants\n",
        "  def GetNbParticipants(self):\n",
        "    return self._nbParticipants\n",
        "  \n",
        "  # Get number of series\n",
        "  def GetNbSeries(self):\n",
        "    return self._nbSeries\n",
        "  \n",
        "  # Get number of exercices per series\n",
        "  def GetNbExercices(self):\n",
        "    return self._nbExercices\n",
        "\n",
        "  # Get total number of exercices\n",
        "  def GetTotalExercices(self):\n",
        "    return self._totalExercices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XGD7hm2qqq1"
      },
      "source": [
        "# DATASET CREATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW9-ZpmHE5bT"
      },
      "source": [
        "## Dataset Class (model)\n",
        "Dataset class that will contain our EEG signals, labels of confusion and possible extracted features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B71QWPZjjpsj"
      },
      "source": [
        "from numpy import savetxt, loadtxt\n",
        "\n",
        "class Dataset:\n",
        "  \"\"\"Class containing our dataset to train our models (labels of confusion, eeg signals and possible extracted features)\"\"\"\n",
        "  def __init__(self):\n",
        "    self._inputList = [] # EEG signals would be saved under a list here\n",
        "    self._inputArray = None # Numpy array of EEG signals\n",
        "    self._normInputArray = None # Normalized numpy array of EEG signals\n",
        "    self._bpArray = None # Bandpower features computed from EEG signals\n",
        "    self._normBpArray = None # # Normalized Bandpower features computed from EEG signals\n",
        "    self._labelList = None # Confusion level labels\n",
        "    self._labelArray = None #Numpy array of confusion level\n",
        "\n",
        "    self._X_train = None\n",
        "    self._X_test = None\n",
        "    self._y_train = None\n",
        "    self._y_test = None\n",
        "\n",
        "  # Set list that contains eeg signals\n",
        "  def SetInputList(self,inputList):\n",
        "    self._inputList = inputList\n",
        "\n",
        "  # Get EEG signals list\n",
        "  def GetInputList(self):\n",
        "    return self._inputList\n",
        "\n",
        "  def SetInputArray(self, arr):\n",
        "    self._inputArray = arr\n",
        "\n",
        "  #Reshape EEG signals array to 2D\n",
        "  def Get2DInputArray(self):\n",
        "    return self._inputArray.reshape(self._inputArray.shape[0], -1)\n",
        "  \n",
        "  #Reshape normalized input array to 2D\n",
        "  def Get2DNormInputArray(self):\n",
        "    return self._normInputArray.reshape(self._normInputArray.shape[0], -1)\n",
        "  \n",
        "  def GetInputArray(self):\n",
        "    return self._inputArray\n",
        "  \n",
        "  #Set normalized input array to arr\n",
        "  def SetNormInputArray(self,arr):\n",
        "    self._normInputArray = arr\n",
        "\n",
        "  #Get normalized input array\n",
        "  def GetNormInputArray(self):\n",
        "    return self._normInputArray\n",
        "  \n",
        "  def SetBpArray(self, arr):\n",
        "    self._bpArray = arr\n",
        "  \n",
        "  def GetBpArray(self):\n",
        "    return self._bpArray\n",
        "  \n",
        "  #Reshape bandpower array to 2D\n",
        "  def Get2DBpArray(self):\n",
        "    return self._bpArray.reshape(self._bpArray.shape[0], -1)\n",
        "  \n",
        "  #Reshape normalized bandpower array to 2D\n",
        "  def Get2DNormBpArray(self):\n",
        "    return self._normBpArray.reshape(self._normBpArray.shape[0], -1)\n",
        "  \n",
        "  #Set normalized bp array to arr\n",
        "  def SetNormBpArray(self,arr):\n",
        "    self._normBpArray = arr\n",
        "\n",
        "  #Get normalized bp array\n",
        "  def GetNormBpArray(self):\n",
        "    return self._normBpArray\n",
        "  \n",
        "  def SetLabelList(self, arr):\n",
        "    self._labelList = arr\n",
        "  \n",
        "  def GetLabelList(self):\n",
        "    return self._labelList\n",
        "  \n",
        "  def SetLabelArray(self, arr):\n",
        "    self._labelArray = arr\n",
        "\n",
        "  def GetLabelArray(self):\n",
        "    return self._labelArray\n",
        "  \n",
        "  #Set splitted dataset (eeg signals or bandpower features)\n",
        "  def SetSplittedDataset(self,X_train, X_test, y_train, y_test):\n",
        "    self._X_train = X_train\n",
        "    self._X_test = X_test\n",
        "    self._y_train = y_train\n",
        "    self._y_test = y_test\n",
        "  \n",
        "  #Get splitted dataset\n",
        "  def GetSplittedDataset(self):\n",
        "    return self._X_train, self._X_test, self._y_train, self._y_test\n",
        "\n",
        "  #Save bandpower features\n",
        "  def SaveBpArray(self):\n",
        "    savetxt('/content/drive/My Drive/Colab Notebooks/Data/CognitiveTest/bpData_conf_bin.csv', self.Get2DBpArray(), delimiter=',')\n",
        "    print(\"Bandpower data saved\")\n",
        "  \n",
        "  #Load bandpower features\n",
        "  def LoadBpArray(self):\n",
        "    data = loadtxt('/content/drive/My Drive/Colab Notebooks/Data/CognitiveTest/bpData_bin.csv', delimiter=',')\n",
        "    data = data.reshape(data.shape[0],1,data.shape[1])\n",
        "    self.SetBpArray(data)\n",
        "    print(\"Bandpower loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hBJW6xQEngb"
      },
      "source": [
        "## DatasetView Class (view)\n",
        "Tools to visualize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3VdB0bC5L-d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(font_scale=1.2)\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "class DatasetView:\n",
        "  \"\"\"Class for dataset visualization\"\"\"\n",
        "  def __init__(self, Dataset):\n",
        "    self._dataset = Dataset\n",
        "  \n",
        "  # printing the list using loop \n",
        "  def PrintInputList(self):\n",
        "    li = self._dataset.GetInputList()\n",
        "    print(len(li))\n",
        "    for x in range(len(li)): \n",
        "      print(li[x])\n",
        "\n",
        "  # printing the list using loop \n",
        "  def PrintLabelList(self):\n",
        "    li = self._dataset.GetLabelList()\n",
        "    print(li)\n",
        "\n",
        "  # printing the array\n",
        "  def PrintInputArray(self):\n",
        "    arr = self._dataset.GetInputArray()\n",
        "    print(\"Array: \", arr)\n",
        "    print(\"Shape: \", arr.shape)\n",
        "\n",
        "  # printing the array\n",
        "  def PrintLabelArray(self):\n",
        "    arr = self._dataset.GetLabelArray()\n",
        "    print(\"Array: \", arr)\n",
        "    print(\"Shape: \", arr.shape)\n",
        "\n",
        "  # Plot the amplitude of each channel for each seconds\n",
        "  def PlotVolt(self,secs):\n",
        "    # Define sampling frequency and time vector\n",
        "    sf = 128.\n",
        "    data = self._dataset.GetInputArray()\n",
        "\n",
        "    #Retrieve data ranged by exercices instead of seconds. So we can later print the volts for the entire exercice\n",
        "    d1 = int(data.shape[0]/secs)\n",
        "    d2 = data.shape[1]*secs\n",
        "    d3 = data.shape[2]\n",
        "    data2 = np.reshape(data,(d1,d2,d3))\n",
        "    print(\"data2 shape: \",data2.shape)\n",
        "\n",
        "    part = 0 #participant number\n",
        "    serie = 0 #serie number\n",
        "    ex = 0 #exercice number\n",
        "    interv = data2.shape[0]\n",
        "    print(interv)\n",
        "    for i in range(interv):\n",
        "      if i%20==0 and i!=0:\n",
        "        part = part+1\n",
        "        serie = 0\n",
        "      if ex==4:\n",
        "        serie = serie + 1\n",
        "        ex = 0\n",
        "      time = np.arange(data2[i].shape[0]) / sf\n",
        "      print(time)\n",
        "      # Plot the signal\n",
        "      plt.plot(time, data2[i], lw=1.5, color='k')\n",
        "      plt.xlabel('Time (seconds)')\n",
        "      plt.ylabel('Voltage')\n",
        "      plt.title('EEG raw signal mV. Participant %i Serie %i Exercice %i ' % (part,serie,ex))\n",
        "      sns.despine()\n",
        "      plt.savefig('/content/drive/My Drive/Colab Notebooks/Data/CognitiveTest/Graphs/'+str(i)+'.png')  # write image to file\n",
        "      plt.clf()\n",
        "\n",
        "      ex = ex+1\n",
        "  \n",
        "  #Visualize dataset in 2D with PCA\n",
        "  @staticmethod\n",
        "  def Pca2D(X,y):\n",
        "    pca = PCA(n_components=2)\n",
        "    arr = X\n",
        "    components = pca.fit_transform(arr)\n",
        "\n",
        "    fig = px.scatter(components, x=0, y=1, color=y)\n",
        "    fig.show()\n",
        "  \n",
        "  #Visualize dataset in 3D with PCA\n",
        "  @staticmethod\n",
        "  def Pca3D(X,y):\n",
        "    pca = PCA(n_components=3)\n",
        "    arr = X\n",
        "    components = pca.fit_transform(arr)\n",
        "\n",
        "    total_var = pca.explained_variance_ratio_.sum() * 100\n",
        "\n",
        "    fig = px.scatter_3d(\n",
        "        components, x=0, y=1, z=2, color=y,\n",
        "        title=f'Total Explained Variance: {total_var:.2f}%',\n",
        "        labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
        "    )\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80CbcsQgF1fW"
      },
      "source": [
        "##WorkerController Class (controller)\n",
        "Class for extracting the results of the cognitive exercises (i.e., labels of confusion) and the EEG signals. This class works like a worker and retrieves the data from the Drive so that the manager (DatasetController) can then manage the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwTj-MZ6GyJz"
      },
      "source": [
        "import pandas.io.common as pio\n",
        "\n",
        "class WorkerController:\n",
        "  \"\"\"Class for extracting the results of the cognitive exercises (with labels of confusion) and the EEG signals. This class will set up ExercicesList and Dataset classes.\"\"\"\n",
        "  def __init__(self, ExercicesList=ExercicesList(), Dataset=Dataset()):\n",
        "    self._exercicesList = ExercicesList\n",
        "    self._dataset = Dataset\n",
        "    self._datasetView = DatasetView(self._dataset)\n",
        "  \n",
        "  def GetExercicesList(self):\n",
        "    return self._exercicesList\n",
        "  \n",
        "  def GetDataset(self):\n",
        "    return self._exercicesList\n",
        "\n",
        "  # Build our input list = load EEG signals from csv files\n",
        "  def BuildInputList(self):\n",
        "    inputList=[]\n",
        "    for f in range(self._exercicesList.GetNbParticipants()):\n",
        "      dataframe = []\n",
        "      for i in range(self._exercicesList.GetNbSeries()):\n",
        "        eegSerie = [] \n",
        "        for j in range(self._exercicesList.GetNbExercices()):\n",
        "          ex = self.GetEEGExercice(f,i,j)\n",
        "          eegSerie.append(ex)\n",
        "        dataframe.append(eegSerie)\n",
        "      inputList.append(dataframe)\n",
        "    self._dataset.SetInputList(inputList)\n",
        "    #self._datasetView.PrintInputList()\n",
        "\n",
        "  # Get EEG signals from an exercice file\n",
        "  def GetEEGExercice(self,participantNb,serieNb,exNb):\n",
        "    participant = participantNb+1\n",
        "    file = self._exercicesList.GetGD().GetUrlFiles()+'P'+str(participant)+'_s'+str(serieNb)+'ex'+str(exNb)+'.csv'\n",
        "    print(file)\n",
        "    try:\n",
        "      test = pd.read_csv(file).fillna(0) #if we can't find the file on the drive (e.g., it has been erased)\n",
        "    except NameError:\n",
        "      print(\"File not Found on drive\")\n",
        "    except pio.EmptyDataError:\n",
        "      print(\"Empty file on drive\")\n",
        "    return test\n",
        "  \n",
        "  # Make sure that each exercise lasts whole seconds (e.g., not half-seconds)\n",
        "  @staticmethod\n",
        "  def CutExercice(data):\n",
        "    isNotReady = True\n",
        "    while isNotReady:  \n",
        "      if(data.shape[0] % 128 != 0):\n",
        "        data = data[:-1]\n",
        "      elif(data.shape[0] % 128 == 0):\n",
        "        isNotReady = False\n",
        "    return data\n",
        "\n",
        "  # Divide each exercise into 14x128 eeg signals per second\n",
        "  @staticmethod\n",
        "  def DivideExercice(data):\n",
        "    nb_secs = int(data.shape[0]/128)\n",
        "    data = data.reshape(nb_secs, 128, 14)\n",
        "    return data\n",
        "\n",
        "  # Keep only X last seconds of the exercice\n",
        "  @staticmethod\n",
        "  def GetXsecs(data,size):\n",
        "    if data.shape[0]>size:\n",
        "      data = data[data.shape[0]-size:]\n",
        "    #else:\n",
        "      #data = np.array([])\n",
        "      #length = size - data.shape[0]\n",
        "      #data = np.pad(data, (0, length), mode='constant')\n",
        "    print(\"shape ex: \",data.shape) \n",
        "    return data\n",
        "\n",
        "  # Normalize exercice: Mean\n",
        "  @staticmethod\n",
        "  def MeanExercice(data):\n",
        "    data = data.reshape(data.shape[1],data.shape[0])\n",
        "    print(data.shape)\n",
        "    data = np.mean(data,axis=1)\n",
        "    data = data.reshape(1,data.shape[0])\n",
        "    print(data)\n",
        "    return data\n",
        "  \n",
        "  #Compute the bandpower for the entire exercice\n",
        "  def ComputeBandpowerMean(data):\n",
        "    data = data.reshape(data.shape[1],data.shape[0])\n",
        "    bpArray = np.zeros((14, 5))\n",
        "    for i in range(data.shape[0]):\n",
        "      sf = data[i].shape[0]\n",
        "      bpArray[i][0] = bandpower(data[i], sf, [1, 4], 'welch') #delta bandpower\n",
        "      bpArray[i][1] = bandpower(data[i], sf, [4, 8], 'welch') #theta bandpower\n",
        "      bpArray[i][2] = bandpower(data[i], sf, [8, 13], 'welch') #alpha bandpower\n",
        "      bpArray[i][3] = bandpower(data[i], sf, [13, 30], 'welch') #beta bandpower\n",
        "      bpArray[i][4] = bandpower(data[i], sf, [30, 43], 'welch') #gamma bandpower\n",
        "    data = bpArray\n",
        "    data = data.reshape(data.shape[1],data.shape[0])\n",
        "    return data\n",
        "  \n",
        "  # Normalize exercice: Cut and divide each exercice in seconds\n",
        "  @staticmethod\n",
        "  def NormalizeExercice(dataframes,participantNb,serieNb,exNb,case):\n",
        "    data_np = np.array(dataframes[participantNb][serieNb][exNb])\n",
        "    data_np = WorkerController.CutExercice(data_np)\n",
        "    if case == 0:\n",
        "      data_np = WorkerController.DivideExercice(data_np)\n",
        "      data_np = WorkerController.GetXsecs(data_np,180) \n",
        "    elif case == 1:\n",
        "      data_np = ComputeBandpowerMean(data_np)\n",
        "    else:\n",
        "      data_np = WorkerController.MeanExercice(data_np)\n",
        "    return data_np\n",
        "  \n",
        "  #Append each array (128x14) to the dataset\n",
        "  @staticmethod\n",
        "  def AppendDataset(data,dataset):\n",
        "    for arr in data:\n",
        "      dataset.append(arr)\n",
        "    return dataset\n",
        "\n",
        " # Cut the EEG signals of the entire exercise into one-second signals and associate the level of confusion of the entire exercise to each second\n",
        "  @staticmethod\n",
        "  def GetExPerSecAndLabels(nbPart, nbSerie, nbEx, countExercices, dataframes, inputs, labels, arrEx, index, total_ex,case,type):\n",
        "    countExercices = countExercices + 1\n",
        "    \n",
        "    # Cut the exercise data\n",
        "    data_np = WorkerController.NormalizeExercice(dataframes,nbPart,nbSerie,nbEx,case=case)\n",
        "\n",
        "    if data_np.size != 0:\n",
        "      inputs = WorkerController.AppendDataset(data_np,inputs)\n",
        "      # Get labels from reported self_confusion\n",
        "      if type == 0: # Binary confusion\n",
        "        end = 1 \n",
        "      else: # 4 levels of confusion\n",
        "        end = 2\n",
        "      for t in range(data_np.shape[0]):\n",
        "        label = arrEx[index][arrEx.shape[1]-end]\n",
        "        labels.append(label)\n",
        "\n",
        "    index = index+1\n",
        "    print(\"We are at \",(countExercices/total_ex)*100, \"%\")\n",
        "    return countExercices, inputs, labels, index\n",
        "  \n",
        "\n",
        "  #Use the input list of EEG signals and the labels extracted from the cognitive exercices results to build the dataset\n",
        "  def BuildDataset(self):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    index = 0\n",
        "    \n",
        "    dataframes = self._dataset.GetInputList()\n",
        "    arrEx = self._exercicesList.GetArrayExercices()\n",
        "    total_ex = self._exercicesList.GetTotalExercices()\n",
        "\n",
        "    countExercices = 0\n",
        "    for f in range(self._exercicesList.GetNbParticipants()):\n",
        "      for i in range(self._exercicesList.GetNbSeries()):\n",
        "        for j in range(self._exercicesList.GetNbExercices()):\n",
        "          countExercices, inputs, labels, index = self.GetExPerSecAndLabels(f,i,j,countExercices, dataframes, inputs, labels, arrEx, index, total_ex,case=0,type=0)\n",
        "\n",
        "    self._dataset.SetInputList(inputs)\n",
        "    self._dataset.SetLabelList(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWfikm3cBkJ1"
      },
      "source": [
        "##DatasetController Class (controller)\n",
        "Now that our dataset has been successfully created by the worker, we can perform action on it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3HV1vfi7yxA"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class DatasetController:\n",
        "  \"\"\"Class that manages the dataset obtained by the WorkerController\"\"\"\n",
        "  def __init__(self, Dataset):\n",
        "    self._dataset = Dataset\n",
        "    self._datasetView = DatasetView(self._dataset)\n",
        "\n",
        "  def ListToNpArray(self):\n",
        "    \"\"\"Transform dataset lists into numpy arrays\"\"\"\n",
        "    self._dataset.SetInputArray(np.array(self._dataset.GetInputList()))\n",
        "    self._dataset.SetLabelArray(np.array(self._dataset.GetLabelList()))\n",
        "    #self._datasetView.PrintInputArray()\n",
        "    #self._datasetView.PrintLabelArray()\n",
        "\n",
        "  def ComputeBandpower(self): \n",
        "    \"\"\"From the raw EEG signals we compute the average bandpower\"\"\"\n",
        "    sf = 128\n",
        "    inputArr =  self._dataset.GetInputArray()\n",
        "    inputArr = np.transpose(inputArr, (0, 2, 1))\n",
        "    #print(inputArr.shape)\n",
        "    bpArray = np.zeros((inputArr.shape[0], 14, 5))\n",
        "\n",
        "    for i in range(inputArr.shape[0]):\n",
        "      for j in range(inputArr.shape[1]):\n",
        "        bpArray[i][j][0] = bandpower(inputArr[i][j], sf, [1, 4], 'welch') #delta bandpower\n",
        "        bpArray[i][j][0] = bandpower(inputArr[i][j], sf, [4, 8], 'welch') #theta bandpower\n",
        "        bpArray[i][j][1] = bandpower(inputArr[i][j], sf, [8, 13], 'welch') #alpha bandpower\n",
        "        bpArray[i][j][2] = bandpower(inputArr[i][j], sf, [13, 30], 'welch') #beta bandpower\n",
        "        bpArray[i][j][3] = bandpower(inputArr[i][j], sf, [30, 43], 'welch') #gamma bandpower\n",
        "      print(\"We are at \",(i/inputArr.shape[0])*100, \"%\")\n",
        "      self._dataset.SetBpArray(bpArray)\n",
        "\n",
        "  # Perform normalized eeg signals array\n",
        "  def PerformNormInput(self):\n",
        "    arr = self._dataset.Get2DInputArray()\n",
        "    arr = stats.zscore(arr)\n",
        "    arr = np.float64(arr)\n",
        "    arr[np.isnan(arr)] = 0\n",
        "    self._dataset.SetNormInputArray(arr)\n",
        "  \n",
        "  # Perform normalized bp array\n",
        "  def PerformNormBp(self):\n",
        "    arr = self._dataset.Get2DBpArray()\n",
        "    arr = stats.zscore(arr)\n",
        "    arr = np.float64(arr)\n",
        "    arr[np.isnan(arr)] = 0\n",
        "    self._dataset.SetNormBpArray(arr)\n",
        "\n",
        "  # Normalize dataset\n",
        "  def NormDataset(self,case):\n",
        "    if case == 0:\n",
        "      self.PerformNormInput()\n",
        "    else:\n",
        "      self.PerformNormBp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WroGGPtGCkui"
      },
      "source": [
        "### Average bandpower\n",
        "\n",
        "FEATURES EXTRACTION: from EEG signal to average bandpower: delta, alpha, beta, theta, gamma https://raphaelvallat.com/bandpower.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks1p6zCkCanl",
        "outputId": "de86756d-8d42-4bda-da95-693a7e4a47dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install mne\n",
        "from scipy import stats\n",
        "from scipy.signal import hilbert, welch\n",
        "from scipy import fftpack\n",
        "from scipy.integrate import simps\n",
        "from mne.time_frequency import psd_array_multitaper\n",
        "\n",
        "def bandpower(data, sf, band, method='welch', window_sec=None, relative=False):\n",
        "    \"\"\"Compute the average power of the signal x in a specific frequency band.\n",
        "\n",
        "    Requires MNE-Python >= 0.14.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : 1d-array\n",
        "      Input signal in the time-domain.\n",
        "    sf : float\n",
        "      Sampling frequency of the data.\n",
        "    band : list\n",
        "      Lower and upper frequencies of the band of interest.\n",
        "    method : string\n",
        "      Periodogram method: 'welch' or 'multitaper'\n",
        "    window_sec : float\n",
        "      Length of each window in seconds. Useful only if method == 'welch'.\n",
        "      If None, window_sec = (1 / min(band)) * 2.\n",
        "    relative : boolean\n",
        "      If True, return the relative power (= divided by the total power of the signal).\n",
        "      If False (default), return the absolute power.\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    bp : float\n",
        "      Absolute or relative band power.\n",
        "    \"\"\"\n",
        "\n",
        "    band = np.asarray(band)\n",
        "    low, high = band\n",
        "\n",
        "    # Compute the modified periodogram (Welch)\n",
        "    if method == 'welch':\n",
        "        if window_sec is not None:\n",
        "            nperseg = window_sec * sf\n",
        "        else:\n",
        "            nperseg = (2 / low) * sf\n",
        "\n",
        "        freqs, psd = welch(data, sf, nperseg=nperseg)\n",
        "\n",
        "    elif method == 'multitaper':\n",
        "        psd, freqs = psd_array_multitaper(data, sf, adaptive=True,\n",
        "                                          normalization='full', verbose=0)\n",
        "\n",
        "    # Frequency resolution\n",
        "    freq_res = freqs[1] - freqs[0]\n",
        "\n",
        "    # Find index of band in frequency vector\n",
        "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
        "\n",
        "    # Integral approximation of the spectrum using parabola (Simpson's rule)\n",
        "    bp = simps(psd[idx_band], dx=freq_res)\n",
        "\n",
        "    if relative:\n",
        "        bp /= simps(psd, dx=freq_res)\n",
        "    return bp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (0.21.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from mne) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mne) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z4vZW2yHTg8"
      },
      "source": [
        "dataset = Dataset()\n",
        "#eL = ExercicesList()\n",
        "#eL.GetListExercices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuQJMRgdHVH0"
      },
      "source": [
        "wc = WorkerController(Dataset=dataset)\n",
        "wc.BuildInputList()\n",
        "wc.BuildDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N0zwR53ZAXB"
      },
      "source": [
        "dc = DatasetController(Dataset=dataset)\n",
        "dc.ListToNpArray()\n",
        "\n",
        "dw = DatasetView(dataset)\n",
        "dw.PrintInputArray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60KTPovMbEh8"
      },
      "source": [
        "dataset.GetLabelArray().shape\n",
        "dataset.GetInputArray().shape\n",
        "dv = DatasetView(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl56a2oMzTE-"
      },
      "source": [
        "#dv.PlotVolt(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xf5Zkqy2aEG"
      },
      "source": [
        "#dc.ComputeBandpower()\n",
        "#dataset.SaveBpArray()\n",
        "dataset.LoadBpArray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0HRLcez6itW"
      },
      "source": [
        "dc.NormDataset(case=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfF5AFQgWkuv"
      },
      "source": [
        "#dv.Pca2D(dataset.GetNormBpArray(),dataset.GetLabelList())\n",
        "#dv.Pca3D(dataset.GetNormBpArray(),dataset.GetLabelList())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLHUFzmCgTnm"
      },
      "source": [
        "# MACHINE LEARNING MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxha-CD9Muxn"
      },
      "source": [
        "## MLModel Class (model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZdWi5yQr42s"
      },
      "source": [
        "class MLModel:\n",
        "  \"\"\"Class containing the common parameters of all Machine learning models\"\"\"\n",
        "  def __init__(self):\n",
        "    self._model = None #will contain the trained model\n",
        "    self._modelAccScore = [] #subset accuracy result of the model on the test set (filled by the controller)\n",
        "    self._modelTrainAccScore = [] #subset accuracy result of the model on the training (filled by the controller)\n",
        "    self._modelPreds = [] #will contain the predictions made by the model\n",
        "    self._confMatrices = []\n",
        "  \n",
        "  #Get the current model\n",
        "  def GetModel(self):\n",
        "    return self._model\n",
        "\n",
        "  #Set the current trained model\n",
        "  def SetModel(self, model):\n",
        "    self._model = model\n",
        "\n",
        "  #Get the current model accuracy\n",
        "  def GetModelAcc(self):\n",
        "    return self._modelAccScore\n",
        "\n",
        "  #Set the accuracy of the model\n",
        "  def SetModelAcc(self,acc):\n",
        "    self._modelAccScore = acc\n",
        "\n",
        "  #Get the current model accuracy\n",
        "  def GetModelTrainAcc(self):\n",
        "    return self._modelTrainAccScore\n",
        "\n",
        "  #Set the accuracy of the model\n",
        "  def SetModelTrainAcc(self,acc):\n",
        "    self._modelTrainAccScore = acc\n",
        "  \n",
        "  #Get the predictions made by the model\n",
        "  def GetModelPreds(self):\n",
        "    return self._modelPreds\n",
        " \n",
        "  #Set the list of predictions made by the model\n",
        "  def SetModelPreds(self,preds):\n",
        "    self._modelPreds = preds\n",
        "\n",
        "  #Set the list of confusions matrices given after the training\n",
        "  def SetConfMatrices(self,confMatrices):\n",
        "    self._confMatrices = confMatrices\n",
        "  \n",
        "  #Get the list of confusions matrices given after the training\n",
        "  def GetConfMatrices(self):\n",
        "    return self._confMatrices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuhzNxqCe8TH"
      },
      "source": [
        "##ViewMLModel Class (view)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgn4AmynU_Gp",
        "outputId": "912c5e51-3146-4fe6-f32e-bca772551b60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install pyriemann\n",
        "from pyriemann.utils.viz import plot_confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "class ViewMLModel:\n",
        "  \"\"\"Class containing the methods for viewing Machine learning results\"\"\"\n",
        "  def __init__(self, Dataset, MLModel):\n",
        "    self._dataset = Dataset\n",
        "    self._mLModel = MLModel\n",
        "\n",
        "  #Get accuracy values for different values of k in KNN\n",
        "  def kRangeAcc(self,k_range):\n",
        "    # plot to see clearly\n",
        "    plt.plot(k_range, self._mLModel.GetModelAcc())\n",
        "    plt.xlabel('Value of K for KNN')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.show()\n",
        "\n",
        "  #Show classification report\n",
        "  def ClassificationReport(self):\n",
        "    Y = self._dataset.GetSplittedDataset()[3]\n",
        "    print(self._mLModel.GetModelAcc())\n",
        "    print(classification_report(Y,self._mLModel.GetModelPreds()))\n",
        "  \n",
        "  #Classification report matrix for lstm model\n",
        "  def ClassificationReportLSTM(self):\n",
        "    print(self._mLModel.GetModelAcc())\n",
        "    Y = self._dataset.GetSplittedDataset()[3]\n",
        "    Y = np_utils.to_categorical(Y,4)\n",
        "    y_pred = self._mLModel.GetModelPreds()\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    Y = np.argmax(Y, axis=1)\n",
        "    print(classification_report(Y,y_pred))\n",
        "  \n",
        "  #General confusion matrix that will be used for KNN and SVM\n",
        "  def ConfMat(self):\n",
        "    preds = self._mLModel.GetModelPreds()\n",
        "    model = self._mLModel.GetModel()\n",
        "\n",
        "    Y = self._dataset.GetSplittedDataset()[3]\n",
        "    # make predictions on the testing data\n",
        "    print(\"[INFO] predicting confusion level ...\")\n",
        "    acc         = np.mean(preds == Y)\n",
        "    print(\"Classification accuracy: %f \" % (acc))\n",
        "\n",
        "    # plot the confusion matrices for both classifiers\n",
        "    names        = [\"Conf0\", \"Conf1\", \"Conf2\", \"Conf3\"]\n",
        "    #names        = possible_labels\n",
        "    plt.figure(0)\n",
        "    plot_confusion_matrix( Y, preds, names, title = str(type(model))+'\\nAccuracy:{0:.3f}'.format(acc))\n",
        "    plt.show()\n",
        "  \n",
        "  #Confusion matrix that will be used for LSTM\n",
        "  def ConfMatLSTM(self):\n",
        "    preds = self._mLModel.GetModelPreds()\n",
        "    Y = self._dataset.GetSplittedDataset()[3]\n",
        "    Y = np_utils.to_categorical(Y,4)\n",
        "    model = self._mLModel.GetModel()\n",
        "\n",
        "    # make predictions on the testing data\n",
        "    print(\"[INFO] predicting confusion level ...\")\n",
        "    preds_       = preds.argmax(axis = -1)\n",
        "    acc         = np.mean(preds_ == Y.argmax(axis=-1))\n",
        "    print(\"Classification accuracy: %f \" % (acc))\n",
        "\n",
        "    # plot the confusion matrices for both classifiers\n",
        "    #names        = [\"Conf0\", \"Conf1\", \"Conf2\", \"Conf3\"]\n",
        "    names        = [\"Conf0\", \"Conf1\"]\n",
        "    #names        = possible_labels\n",
        "    plt.figure(0)\n",
        "    plot_confusion_matrix(Y.argmax(axis = -1), preds_, names, title = str(type(model))+'\\nAccuracy:{0:.3f}'.format(acc))\n",
        "    plt.show()\n",
        "  \n",
        "  #Confusion matrix obtained after CV\n",
        "  def ConfMatCV(self):\n",
        "    acc = self._mLModel.GetModelAcc()\n",
        "    acc = np.array(acc)\n",
        "    std = np.std(acc)\n",
        "    acc = np.mean(acc)\n",
        "    confMatrices =  self._mLModel.GetConfMatrices()\n",
        "    # Transform to df for easier plotting\n",
        "    cm_df = pd.DataFrame(confMatrices,\n",
        "                        index = ['Conf0','Conf1'], \n",
        "                        columns = ['Conf0','Conf1'])\n",
        "                        #index = ['Conf0','Conf1','Conf2','Conf3'], \n",
        "                        #columns = ['Conf0','Conf1','Conf2','Conf3'])\n",
        "\n",
        "    plt.figure(figsize=(5.5,4))\n",
        "    sns.heatmap(cm_df, annot=True, cmap='Blues')\n",
        "    plt.title(str(type(self._mLModel.GetModel()))+'\\nAccuracy:{0:.3f}'.format(acc) +'\\nStd:{0:.3f}'.format(std))\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "  \n",
        "  #Plot the validation curve for hyperparameters\n",
        "  def ValidationCurve(self,params,paramName):\n",
        "    plt.style.use('ggplot')\n",
        "    scores = self._mLModel.GetModelAcc()\n",
        "    scores = np.array(scores)\n",
        "    scores_train = self._mLModel.GetModelTrainAcc()\n",
        "    scores_train = np.array(scores_train)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.plot(params, np.mean(scores_train,axis=1), alpha=0.5, color='blue', label='train')\n",
        "    ax.plot(params, np.mean(scores,axis=1), alpha=0.5, color='red', label='cv')\n",
        "    ax.fill_between(params, np.mean(scores,axis=1) - np.std(scores,axis=1), \n",
        "                np.mean(scores,axis=1) + np.std(scores,axis=1), color='#888888', alpha=0.4)\n",
        "    ax.fill_between(params, np.mean(scores,axis=1) - 2*np.std(scores,axis=1), \n",
        "                np.mean(scores,axis=1) + 2*np.std(scores,axis=1), color='#888888', alpha=0.2)\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_ylabel(\"Accuracy\")\n",
        "    ax.set_xlabel(paramName);\n",
        "  \n",
        "  #Plot the validation curve of the LSTM\n",
        "  @staticmethod\n",
        "  def PlotLearningCurveLSTM(histo):\n",
        "    #print(histo.history.keys())\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(histo.history['accuracy'])\n",
        "    plt.plot(histo.history['val_accuracy'])\n",
        "    plt.plot(histo.history['loss'])\n",
        "    plt.plot(histo.history['val_loss'])\n",
        "    plt.title('model accuracy')\n",
        "    #plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train accuracy', 'validation accuracy','train loss','validation loss'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  #CV learning curve from https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
        "  def plot_learning_curve(self, axes=None, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    estimator = self._mLModel.GetModel()\n",
        "    title = str(type(estimator))+ 'learning curve'\n",
        "    X = self._dataset.GetSplittedDataset()[0]\n",
        "    y = self._dataset.GetSplittedDataset()[3]\n",
        "\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyriemann in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyriemann) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyriemann) (1.18.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyriemann) (0.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyriemann) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyriemann) (1.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pyriemann) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pyriemann) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->pyriemann) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2ESZPkMfR9c"
      },
      "source": [
        "##MLModelController Class (Controller)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XdeIFWoWXgs"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, Dropout, GaussianNoise\n",
        "from keras.layers import Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from keras import callbacks \n",
        "\n",
        "class MLModelController:\n",
        "  \"\"\"Class that will perform actions on model \"\"\"\n",
        "  def __init__(self,Dataset,MLModel=MLModel()):\n",
        "    self._dataset = Dataset\n",
        "    self._mLModel = MLModel\n",
        "    self._viewMlModel = ViewMLModel(Dataset,MLModel)\n",
        "\n",
        "  @staticmethod\n",
        "  def ComputeConfMatrices(confMatrices):\n",
        "    \"\"\"Method that will perform the mean of all saved matrices of confusion \"\"\"\n",
        "    confMatrices = np.array(confMatrices)\n",
        "    confMatrices = np.mean(confMatrices,axis=0)\n",
        "    confMatrices = confMatrices.astype('float') / confMatrices.sum(axis=1)[:, np.newaxis]\n",
        "    return confMatrices\n",
        "  \n",
        "  def SaveInterModelResults(self,knn,y_pred,score,X_train, X_valid, y_train, y_valid):\n",
        "    \"\"\"Method that save the temporary results of the models \"\"\"\n",
        "    self._mLModel.SetModel(knn)\n",
        "    self._mLModel.SetModelPreds(y_pred)\n",
        "    self._mLModel.SetModelAcc(score)\n",
        "    self._dataset.SetSplittedDataset(X_train, X_valid, y_train, y_valid)\n",
        "  \n",
        "  def SaveFinalModelResults(self,y_preds,scores,accScores,confMatrices):\n",
        "    \"\"\"Method that save the results of the models \"\"\"\n",
        "    self._mLModel.SetModelPreds(y_preds)\n",
        "    self._mLModel.SetModelAcc(scores)\n",
        "    self._mLModel.SetModelTrainAcc(accScores)\n",
        "    confMatrices = MLModelController.ComputeConfMatrices(confMatrices)\n",
        "    self._mLModel.SetConfMatrices(confMatrices)\n",
        "    \n",
        "  def KnnModel(self, case=1):\n",
        "    \"\"\"Perform KNN algorithm and evaluate the model on validation or testing set according to case value\"\"\"\n",
        "    if case == 0:\n",
        "      X = self._dataset.Get2DNormInputArray()\n",
        "    else:\n",
        "      X = self._dataset.Get2DNormBpArray()\n",
        "    \n",
        "    y = self._dataset.GetLabelArray()\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state = 42)\n",
        "\n",
        "    scores = []\n",
        "    scores_train = []\n",
        "    confMatrices = []\n",
        "    y_preds = []\n",
        "\n",
        "    final_scores_train = []\n",
        "    final_scores = []\n",
        "\n",
        "    ks = range(1,20)\n",
        "\n",
        "    for k in ks:\n",
        "      for train_idx, valid_idx, in cv.split(X, y):\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
        "        self.PerformKNN1CV(X,y,X_train,y_train,X_valid,y_valid,k,scores_train,scores,y_preds,confMatrices)\n",
        "\n",
        "    \n",
        "      self.SaveFinalModelResults(y_preds,scores,scores_train,confMatrices)\n",
        "      self._viewMlModel.ConfMatCV()\n",
        "      final_scores_train.append(scores_train)\n",
        "      final_scores.append(scores)\n",
        "      scores = []\n",
        "      scores_train = []\n",
        "    \n",
        "    self.SaveFinalModelResults(y_preds,final_scores,final_scores_train,confMatrices)\n",
        "    self._viewMlModel.ValidationCurve(ks,\"k\")\n",
        "  \n",
        "  def PerformKNN1CV(self,X,y,X_train,y_train,X_valid,y_valid,k,scores_train,scores,y_preds,confMatrices):\n",
        "    \"\"\"Perform KNN algorithm for 1 CV\"\"\"\n",
        "\n",
        "    #Oversample data\n",
        "    smt = SMOTE(random_state=0)\n",
        "    X_train, y_train = smt.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(sorted(Counter(y_train).items()))\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    score2 = knn.score(X_train, y_train)\n",
        "    print(\"Training score: \",score2)\n",
        "    scores_train.append(score2)\n",
        "\n",
        "    y_pred=knn.predict(X_valid)\n",
        "    score = metrics.accuracy_score(y_valid,y_pred)\n",
        "    scores.append(score)\n",
        "\n",
        "    print(\"done for k=\",k)\n",
        "\n",
        "    self.SaveInterModelResults(knn,y_pred,score,X_train,X_valid,y_train,y_valid)\n",
        "    self._viewMlModel.ClassificationReport()\n",
        "    #self._viewMlModel.ConfMat()\n",
        "\n",
        "    y_preds.append(y_pred)\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_valid, knn.predict(X_valid))\n",
        "    confMatrices.append(conf_matrix)\n",
        "  \n",
        "  def LstmModel(self):\n",
        "    \"\"\"Perform LSTM algorithm and evaluate the model on validation or testing set according to case value\"\"\"\n",
        "    X_train = self._dataset.GetSplittedDataset()[0]\n",
        "   \n",
        "    y_train = self._dataset.GetSplittedDataset()[2]\n",
        "    X_valid = self._dataset.GetSplittedDataset()[1]\n",
        "    y_valid = self._dataset.GetSplittedDataset()[3]\n",
        "\n",
        "    dv.Pca3D(X_train,y_train)\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(sorted(Counter(y_train).items()))\n",
        "    X_valid = X_valid.reshape(X_valid.shape[0],1,X_valid.shape[1])\n",
        "\n",
        "    batch_size= 128\n",
        "    hidden_size= 47\n",
        "    num_epochs = 1000\n",
        "    num_classes = 5\n",
        "\n",
        "    y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "    y_valid = np_utils.to_categorical(y_valid,num_classes)\n",
        "\n",
        "    print(X_train.shape)\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hidden_size, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units = num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    opt = Adam(lr=0.001)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    print(model.summary())\n",
        "\n",
        "    #earlyStop=EarlyStopping(monitor=\"val_accuracy\",verbose=1,mode='max',patience=10)\n",
        "    earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\",  \n",
        "                                        mode =\"min\", patience = 5,  \n",
        "                                        restore_best_weights = True) \n",
        "    history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_valid, y_valid), verbose=1)#, callbacks =[earlystopping])\n",
        "    model2 = model\n",
        "    y_pred = model2.predict(X_valid, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    score = model.evaluate(X_valid,y_valid, verbose=2,batch_size=batch_size)\n",
        "\n",
        "    acc = np.mean(y_pred.argmax(axis = -1) == y_valid.argmax(axis=-1))\n",
        "\n",
        "    self._mLModel.SetModel(model)\n",
        "    self._mLModel.SetModelPreds(y_pred)\n",
        "    self._mLModel.SetModelAcc(acc)\n",
        "    self._viewMlModel.ClassificationReportLSTM()\n",
        "    self._viewMlModel.ConfMatLSTM()\n",
        "\n",
        "    self._viewMlModel.PlotLearningCurveLSTM(history)\n",
        "\n",
        "  def SVMModel(self,case=1):\n",
        "    \"\"\"Perform SVM algorithm and evaluate the model on validation or testing set according to case value\"\"\"\n",
        "    if case == 0:\n",
        "      X = self._dataset.Get2DNormInputArray()\n",
        "    else:\n",
        "      X = self._dataset.Get2DNormBpArray()\n",
        "    \n",
        "    y = self._dataset.GetLabelArray()\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state = 42)\n",
        "\n",
        "    scores = []\n",
        "    scores_train = []\n",
        "    confMatrices = []\n",
        "    y_preds = []\n",
        "    \n",
        "    Cs = [0.01,0.1,1,10,100]\n",
        "    final_scores_train = []\n",
        "    final_scores = []\n",
        "    gammas = 100\n",
        "\n",
        "    for C in Cs:\n",
        "      for train_idx, valid_idx, in cv.split(X, y):\n",
        "        X_train, y_train = X[train_idx], y[train_idx]\n",
        "        X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
        "        self.PerformSVM1CV(X,y,X_train,y_train,X_valid,y_valid,gammas,C,scores_train,scores,y_preds,confMatrices)\n",
        "      \n",
        "      self.SaveFinalModelResults(y_preds,scores,scores_train,confMatrices)\n",
        "      self._viewMlModel.ConfMatCV()\n",
        "      final_scores_train.append(scores_train)\n",
        "      final_scores.append(scores)\n",
        "      scores = []\n",
        "      scores_train = []\n",
        "    \n",
        "    self.SaveFinalModelResults(y_preds,final_scores,final_scores_train,confMatrices)\n",
        "    self._viewMlModel.ValidationCurve(Cs,\"C\")\n",
        "\n",
        "  def PerformSVM1CV(self,X,y,X_train,y_train,X_valid,y_valid,gamma,C,scores_train,scores,y_preds,confMatrices):\n",
        "    \"\"\"Perform SVM algorithm for 1 CV\"\"\"\n",
        "\n",
        "    #Oversample data\n",
        "    smt = SMOTE(random_state=0)\n",
        "    X_train, y_train = smt.fit_resample(X_train, y_train)\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(sorted(Counter(y_train).items()))\n",
        "\n",
        "    svm = make_pipeline(SVC(gamma=gamma, C = C))\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    score2 = svm.score(X_train, y_train)\n",
        "    print(\"Training score: \",score2)\n",
        "    scores_train.append(score2)\n",
        "\n",
        "    y_pred=svm.predict(X_valid)\n",
        "    score = metrics.accuracy_score(y_valid,y_pred)\n",
        "    scores.append(score)\n",
        "\n",
        "    self.SaveInterModelResults(svm,y_pred,score,X_train,X_valid,y_train,y_valid)\n",
        "    self._viewMlModel.ClassificationReport()\n",
        "\n",
        "    #self._viewMlModel.ConfMat()\n",
        "    y_preds.append(y_pred)\n",
        "    conf_matrix = confusion_matrix(y_valid, svm.predict(X_valid))\n",
        "    confMatrices.append(conf_matrix)\n",
        "    #self._viewMlModel.plot_learning_curve()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxhp7nLgSyBf"
      },
      "source": [
        "mld = MLModelController(Dataset=dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDDwVO-x_fg-"
      },
      "source": [
        "mld.KnnModel(case=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4Bz2p4E4okG"
      },
      "source": [
        "mld.LstmModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qliD-iT_WAjJ"
      },
      "source": [
        "mld.SVMModel(case=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}